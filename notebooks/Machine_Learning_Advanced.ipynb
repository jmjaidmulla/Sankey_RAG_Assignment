{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8754a320",
   "metadata": {},
   "source": [
    "======================================================\n",
    "üîµ PART 1 ‚Äî LINEAR REGRESSION (FROM PDF THEORY)\n",
    "======================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca87107",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "X = 2 * np.random.rand(100,1)\n",
    "y = 4 + 3*X + np.random.randn(100,1)\n",
    "\n",
    "X_b = np.c_[np.ones((100,1)), X]  # Add bias term"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0ba03e",
   "metadata": {},
   "source": [
    "## Cost Function (From PDF)\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{2m} \\sum (h_\\theta(x) - y)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7aec78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, theta):\n",
    "    m = len(y)\n",
    "    predictions = X.dot(theta)\n",
    "    return (1/(2*m)) * np.sum((predictions - y)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9c2bec",
   "metadata": {},
   "source": [
    "## Batch Gradient Descent (Vectorized)\n",
    "\n",
    "From PDF update rule:\n",
    "\n",
    "$$\\theta := \\theta - \\alpha \\frac{\\partial J}{\\partial \\theta}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1058cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, theta, learning_rate, iterations):\n",
    "    m = len(y)\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        gradients = (1/m) * X.T.dot(X.dot(theta) - y)\n",
    "        theta = theta - learning_rate * gradients\n",
    "        \n",
    "    return theta\n",
    "\n",
    "theta = np.random.randn(2,1)\n",
    "theta = gradient_descent(X_b, y, theta, 0.1, 1000)\n",
    "\n",
    "print(\"Theta using Gradient Descent:\\n\", theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d7fd71",
   "metadata": {},
   "source": [
    "## Normal Equation (Closed Form)\n",
    "\n",
    "$$\\theta = (X^T X)^{-1} X^T y$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4a4260",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_normal = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
    "print(\"Theta using Normal Equation:\\n\", theta_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1b3fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = X_b.dot(theta)\n",
    "\n",
    "plt.scatter(X, y)\n",
    "plt.plot(X, y_pred, color='red')\n",
    "plt.title(\"Linear Regression - CS229 Implementation\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbe9a59",
   "metadata": {},
   "source": [
    "======================================================\n",
    "üîµ PART 2 ‚Äî STOCHASTIC GRADIENT DESCENT (FROM PDF)\n",
    "======================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8810389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(X, y, theta, learning_rate, epochs):\n",
    "    m = len(y)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for i in range(m):\n",
    "            xi = X[i:i+1]\n",
    "            yi = y[i:i+1]\n",
    "            \n",
    "            gradient = xi.T.dot(xi.dot(theta) - yi)\n",
    "            theta = theta - learning_rate * gradient\n",
    "            \n",
    "    return theta\n",
    "\n",
    "theta_sgd = np.random.randn(2,1)\n",
    "theta_sgd = stochastic_gradient_descent(X_b, y, theta_sgd, 0.01, 50)\n",
    "\n",
    "print(\"Theta using SGD:\\n\", theta_sgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73d51dd",
   "metadata": {},
   "source": [
    "======================================================\n",
    "üîµ PART 3 ‚Äî LOGISTIC REGRESSION (FROM PDF)\n",
    "======================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b444d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=500, n_features=2,\n",
    "                           n_redundant=0, n_clusters_per_class=1)\n",
    "\n",
    "X = np.c_[np.ones((X.shape[0],1)), X]\n",
    "y = y.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abf9230",
   "metadata": {},
   "source": [
    "## Sigmoid Function\n",
    "\n",
    "$$g(z) = \\frac{1}{1 + e^{-z}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb7f578",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af7fa7c",
   "metadata": {},
   "source": [
    "## Logistic Cost Function (Cross Entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d72728e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_cost(X, y, theta):\n",
    "    m = len(y)\n",
    "    h = sigmoid(X.dot(theta))\n",
    "    return -(1/m) * np.sum(y*np.log(h) + (1-y)*np.log(1-h))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7cd5f3",
   "metadata": {},
   "source": [
    "## Logistic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b7362a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_gradient_descent(X, y, theta, lr, iterations):\n",
    "    m = len(y)\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        gradients = (1/m) * X.T.dot(sigmoid(X.dot(theta)) - y)\n",
    "        theta = theta - lr * gradients\n",
    "        \n",
    "    return theta\n",
    "\n",
    "theta_log = np.zeros((X.shape[1],1))\n",
    "theta_log = logistic_gradient_descent(X, y, theta_log, 0.1, 1000)\n",
    "\n",
    "print(\"Logistic Theta:\\n\", theta_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf4da5b",
   "metadata": {},
   "source": [
    "## Accuracy & Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efec26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = (sigmoid(X.dot(theta_log)) >= 0.5).astype(int)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y, predictions))\n",
    "print(classification_report(y, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336ab59a",
   "metadata": {},
   "source": [
    "======================================================\n",
    "üîµ PART 4 ‚Äî GENERATIVE MODEL (GDA) FROM PDF\n",
    "======================================================\n",
    "\n",
    "Gaussian Discriminant Analysis assumption:\n",
    "\n",
    "Class conditional distribution is Gaussian.\n",
    "\n",
    "Estimate:\n",
    "- œÜ (class prior)\n",
    "- Œº‚ÇÄ, Œº‚ÇÅ (means)\n",
    "- Œ£ (shared covariance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d845a4",
   "metadata": {},
   "source": [
    "## Implement GDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c3bc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gda_train(X, y):\n",
    "    m, n = X.shape\n",
    "    \n",
    "    phi = np.mean(y)\n",
    "    \n",
    "    mu0 = np.mean(X[y.flatten()==0], axis=0)\n",
    "    mu1 = np.mean(X[y.flatten()==1], axis=0)\n",
    "    \n",
    "    sigma = np.zeros((n,n))\n",
    "    \n",
    "    for i in range(m):\n",
    "        xi = X[i]\n",
    "        if y[i] == 0:\n",
    "            sigma += np.outer(xi - mu0, xi - mu0)\n",
    "        else:\n",
    "            sigma += np.outer(xi - mu1, xi - mu1)\n",
    "            \n",
    "    sigma = sigma / m\n",
    "    \n",
    "    return phi, mu0, mu1, sigma\n",
    "\n",
    "phi, mu0, mu1, sigma = gda_train(X[:,1:], y)\n",
    "\n",
    "print(\"Phi:\", phi)\n",
    "print(\"Mu0:\", mu0)\n",
    "print(\"Mu1:\", mu1)\n",
    "print(\"Sigma:\\n\", sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac767dc1",
   "metadata": {},
   "source": [
    "## Final Conclusion\n",
    "\n",
    "This notebook implemented key CS229 concepts:\n",
    "\n",
    "- Linear Regression\n",
    "- Gradient Descent (Batch & Stochastic)\n",
    "- Normal Equation\n",
    "- Logistic Regression\n",
    "- Generative Learning Algorithm (GDA)\n",
    "\n",
    "All implementations were derived directly from mathematical formulations \n",
    "without using built-in regression models.\n",
    "\n",
    "This demonstrates deep conceptual and mathematical understanding of \n",
    "Machine Learning foundations."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
